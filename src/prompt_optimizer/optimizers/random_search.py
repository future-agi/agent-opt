import litellm
from typing import List, Dict, Any
from ..base import BaseOptimizer, OptimizationResult, DataMapper, BaseGenerator

class RandomSearchOptimizer(BaseOptimizer):
    """
    A simple optimization strategy that tries a number of random prompt variations
    generated by a powerful "teacher" model.
    """

    def __init__(self, generator: BaseGenerator, teacher_model: str = "gpt-5", num_variations: int = 5, teacher_model_kwargs: Dict[str, Any] = None, eval_template: str = "summary_quality", eval_model_name: str = "turing_flash"):
        """
        Initializes the Random Search Optimizer.

        Args:
            generator: The generator to be optimized.
            teacher_model: A powerful LLM to generate prompt variations.
            num_variations: The number of random variations to try.
            teacher_model_kwargs: A dictionary of kwargs to pass to the teacher model.
            eval_template: The evaluation template to use from the ai-evaluation library.
            eval_model_name: The model to use for evaluation.
        """
        self.generator = generator
        self.teacher_model = teacher_model
        self.num_variations = num_variations
        self.eval_template = eval_template
        self.eval_model_name = eval_model_name
        
        if teacher_model_kwargs is None and "gpt" in teacher_model:
             self.teacher_model_kwargs = {"temperature": 1.0, "max_tokens": 16000}
        else:
            self.teacher_model_kwargs = teacher_model_kwargs or {}

    def optimize(
        self,
        evaluator: Any,
        data_mapper: DataMapper,
        dataset: List[Dict[str, Any]],
        **kwargs: Any
    ) -> OptimizationResult:
        
        print("--- Starting Random Search Optimization ---")
        
        initial_prompt = self.generator.get_prompt_template()
        best_prompt = initial_prompt
        best_score = -1.0
        history = []

        variations = self._generate_variations(initial_prompt)
        
        for i, variation in enumerate(variations):
            print(f"\n--- Testing Variation {i+1}/{len(variations)} ---")
            print(f"Prompt: {variation}")
            
            self.generator.set_prompt_template(variation)
            
            scores = []
            for example in dataset:
                generated_output = self.generator.generate(example)
                eval_inputs = data_mapper.map(generated_output, example)
                
                eval_result = evaluator.evaluate(
                    eval_templates=self.eval_template,
                    inputs=eval_inputs,
                    model_name=self.eval_model_name
                )
                
                if eval_result.eval_results:
                    scores.append(eval_result.eval_results[0].output)

            if not scores:
                continue

            avg_score = sum(scores) / len(scores)
            print(f"Average Score: {avg_score:.4f}")
            
            history.append({"prompt": variation, "score": avg_score})

            if avg_score > best_score:
                best_score = avg_score
                best_prompt = variation
                print("--- New Best Prompt Found! ---")

        self.generator.set_prompt_template(best_prompt)
        
        return OptimizationResult(
            best_generator=self.generator,
            history=history,
            final_score=best_score
        )

    def _generate_variations(self, initial_prompt: str) -> List[str]:
        print(f"Generating {self.num_variations} prompt variations with teacher model: {self.teacher_model}...")
        
        instruction = f"""
        You are an expert in prompt engineering. Your task is to generate {self.num_variations} variations of the following prompt.
        The variations should be diverse and explore different styles.
        Return ONLY a Python list of the strings, like ["prompt 1", "prompt 2", ...].

        Initial Prompt:
        ---
        {initial_prompt}
        ---
        """
        messages = [{"role": "user", "content": instruction}]
        
        try:
            response = litellm.completion(
                model=self.teacher_model,
                messages=messages,
                **self.teacher_model_kwargs
            )
            response_content = response.choices[0].message.content
            variations = eval(response_content)
            if isinstance(variations, list) and all(isinstance(i, str) for i in variations):
                return variations
            else:
                raise ValueError("Teacher model did not return a valid list of strings.")
        except Exception as e:
            print(f"An error occurred with the teacher model: {e}")
            return [initial_prompt + " Be creative."]
