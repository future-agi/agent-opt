import litellm
import logging
import time
from typing import List, Dict, Any
from ..base.base_optimizer import BaseOptimizer
from ..base.base_generator import BaseGenerator

from ..datamappers import BasicDataMapper
from ..types import OptimizationResult


class RandomSearchOptimizer(BaseOptimizer):
    """
    A simple optimization strategy that tries a number of random prompt variations
    generated by a powerful "teacher" model.
    """

    def __init__(
        self,
        generator: BaseGenerator,
        teacher_model: str = "gpt-5",
        num_variations: int = 5,
        teacher_model_kwargs: Dict[str, Any] = {},
        eval_template: str = "summary_quality",
        eval_model_name: str = "turing_flash",
    ):
        """
        Initializes the Random Search Optimizer.

        Args:
            generator: The generator to be optimized.
            teacher_model: A powerful LLM to generate prompt variations.
            num_variations: The number of random variations to try.
            teacher_model_kwargs: A dictionary of kwargs to pass to the teacher model.
            eval_template: The evaluation template to use from the ai-evaluation library.
            eval_model_name: The model to use for evaluation.
        """
        self.generator = generator
        self.teacher_model = teacher_model
        self.num_variations = num_variations
        self.eval_template = eval_template
        self.eval_model_name = eval_model_name

        if teacher_model_kwargs is None and "gpt" in teacher_model:
            self.teacher_model_kwargs = {"temperature": 1.0, "max_tokens": 16000}
        else:
            self.teacher_model_kwargs = teacher_model_kwargs or {}

    def optimize(
        self,
        evaluator: Any,
        data_mapper: BasicDataMapper,
        dataset: List[Dict[str, Any]],
        **kwargs: Any,
    ) -> OptimizationResult:
        logging.info("--- Starting Random Search Optimization ---")

        initial_prompt = self.generator.get_prompt_template()
        best_prompt = initial_prompt
        best_score = -1.0
        history = []

        variations = self._generate_variations(initial_prompt)

        for i, variation in enumerate(variations):
            iteration_start_time = time.time()
            logging.info(f"--- Testing Variation {i + 1}/{len(variations)} ---")
            logging.info(f"Prompt: {variation}")

            self.generator.set_prompt_template(variation)

            scores = []
            total_evaluation_time = 0
            total_generation_time = 0

            for example_idx, example in enumerate(dataset):
                logging.info(f"Processing example {example_idx + 1}/{len(dataset)}")

                # Time generation
                generation_start_time = time.time()
                generated_output = self.generator.generate(example)
                generation_end_time = time.time()
                gen_time = generation_end_time - generation_start_time
                total_generation_time += gen_time
                logging.info(f"Generation took {gen_time:.2f} seconds.")

                eval_inputs = data_mapper.map(generated_output, example)

                # Time evaluation
                evaluation_start_time = time.time()
                eval_result = evaluator.evaluate(
                    eval_templates=self.eval_template,
                    inputs=eval_inputs,
                    model_name=self.eval_model_name,
                )
                evaluation_end_time = time.time()
                eval_time = evaluation_end_time - evaluation_start_time
                total_evaluation_time += eval_time
                logging.info(f"Evaluation took {eval_time:.2f} seconds.")

                if eval_result.eval_results:
                    score = eval_result.eval_results[0].output
                    scores.append(score)
                    logging.info(f"Score for example {example_idx + 1}: {score:.4f}")
                else:
                    logging.warning(
                        f"No evaluation result for example {example_idx + 1}"
                    )

            if not scores:
                logging.warning(
                    "No scores were recorded for this variation. Skipping."
                )
                continue

            avg_score = sum(scores) / len(scores)
            logging.info(f"Average Score: {avg_score:.4f}")
            logging.info(
                f"Total generation time for this variation: {total_generation_time:.2f}s"
            )
            logging.info(
                f"Total evaluation time for this variation: {total_evaluation_time:.2f}s"
            )

            history.append({"prompt": variation, "score": avg_score})

            if avg_score > best_score:
                best_score = avg_score
                best_prompt = variation
                logging.info("--- New Best Prompt Found! ---")

            iteration_end_time = time.time()
            logging.info(
                f"--- Variation {i + 1} finished in {iteration_end_time - iteration_start_time:.2f} seconds ---"
            )

        self.generator.set_prompt_template(best_prompt)

        return OptimizationResult(
            best_generator=self.generator,
            message_history=history,
            final_score=best_score,
        )

    def _generate_variations(self, initial_prompt: str) -> List[str]:
        logging.info(
            f"Generating {self.num_variations} prompt variations with teacher model: {self.teacher_model}..."
        )

        instruction = f"""
        You are an expert in prompt engineering. Your task is to generate {self.num_variations} variations of the following prompt.
        The variations should be diverse and explore different styles.
        Return ONLY a Python list of the strings, like ["prompt 1", "prompt 2", ...].

        Initial Prompt:
        ---
        {initial_prompt}
        ---
        """
        messages = [{"role": "user", "content": instruction}]

        try:
            response = litellm.completion(
                model=self.teacher_model,
                messages=messages,
                **self.teacher_model_kwargs,
            )
            response_content = response.choices[0].message.content
            variations = eval(response_content)
            if isinstance(variations, list) and all(
                isinstance(i, str) for i in variations
            ):
                return variations
            else:
                raise ValueError(
                    "Teacher model did not return a valid list of strings."
                )
        except Exception as e:
            logging.error(f"An error occurred with the teacher model: {e}")
            return [initial_prompt + " Be creative."]
