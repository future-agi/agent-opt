import random
from typing import List, Dict, Any
from ..base import BaseStrategy, BaseGenerator, OptimizationResult, DataMapper

class RandomSearchStrategy(BaseStrategy):
    """
    A simple optimization strategy that tries a number of random prompt variations
    generated by a powerful "teacher" model.
    """

    def __init__(self, teacher_model: str = "gpt-5", num_variations: int = 5):
        """
        Initializes the Random Search Strategy.

        Args:
            teacher_model: A powerful LLM to generate prompt variations.
            num_variations: The number of random variations to try.
        """
        self.teacher_model = teacher_model
        self.num_variations = num_variations

    def optimize(
        self,
        generator: BaseGenerator,
        evaluator: Any,
        data_mapper: DataMapper,
        trainset: List[Dict[str, Any]],
        valset: List[Dict[str, Any]]
    ) -> OptimizationResult:
        
        print("--- Starting Random Search Optimization ---")
        
        initial_prompt = generator.get_prompt_template()
        best_prompt = initial_prompt
        best_score = -1.0
        history = []

        # Generate variations of the initial prompt
        variations = self._generate_variations(initial_prompt)
        
        for i, variation in enumerate(variations):
            print(f"\n--- Testing Variation {i+1}/{self.num_variations} ---")
            print(f"Prompt: {variation}")
            
            generator.set_prompt_template(variation)
            
            # Evaluate the new prompt on the validation set
            scores = []
            for example in valset:
                generated_output = generator.generate(example)
                eval_inputs = data_mapper.map(generated_output, example)
                
                # Assuming the evaluator is from fi.evals
                eval_result = evaluator.evaluate(
                    eval_templates="summary_quality", # This could be made configurable
                    inputs=eval_inputs
                )
                
                if eval_result.eval_results:
                    scores.append(eval_result.eval_results[0].output)

            if not scores:
                continue

            avg_score = sum(scores) / len(scores)
            print(f"Average Score: {avg_score:.4f}")
            
            history.append({"prompt": variation, "score": avg_score})

            if avg_score > best_score:
                best_score = avg_score
                best_prompt = variation
                print("--- New Best Prompt Found! ---")

        # Set the generator to the best prompt found
        generator.set_prompt_template(best_prompt)
        
        return OptimizationResult(
            best_generator=generator,
            history=history,
            final_score=best_score
        )

    def _generate_variations(self, initial_prompt: str) -> List[str]:
        # This is a placeholder for a call to the teacher model.
        # In a real implementation, you would use LiteLLM to ask the teacher model
        # to generate N variations of the initial_prompt.
        print("Generating prompt variations with teacher model...")
        return [
            initial_prompt + " Be creative.",
            initial_prompt + " Be concise and to the point.",
            initial_prompt + " Write in a professional tone.",
            initial_prompt + " Explain it like I'm five.",
            initial_prompt + " Use a narrative style.",
        ]
