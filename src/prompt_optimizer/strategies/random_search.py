import litellm
import random
from typing import List, Dict, Any
from ..base import BaseStrategy, BaseGenerator, OptimizationResult, DataMapper

class RandomSearchStrategy(BaseStrategy):
    """
    A simple optimization strategy that tries a number of random prompt variations
    generated by a powerful "teacher" model.
    """

    def __init__(self, teacher_model: str = "gpt-5", num_variations: int = 5, teacher_model_kwargs: Dict[str, Any] = None):
        """
        Initializes the Random Search Strategy.

        Args:
            teacher_model: A powerful LLM to generate prompt variations.
            num_variations: The number of random variations to try.
            teacher_model_kwargs: A dictionary of kwargs to pass to the teacher model,
                                 e.g., {"temperature": 1.0, "max_tokens": 16000}.
        """
        self.teacher_model = teacher_model
        self.num_variations = num_variations
        
        # Set default kwargs for reasoning models if none are provided
        if teacher_model_kwargs is None and "gpt" in teacher_model:
             self.teacher_model_kwargs = {"temperature": 1.0, "max_tokens": 16000}
        else:
            self.teacher_model_kwargs = teacher_model_kwargs or {}


    def optimize(
        self,
        generator: BaseGenerator,
        evaluator: Any,
        data_mapper: DataMapper,
        trainset: List[Dict[str, Any]],
        valset: List[Dict[str, Any]],
    ) -> OptimizationResult:
        
        print("--- Starting Random Search Optimization ---")
        
        initial_prompt = generator.get_prompt_template()
        best_prompt = initial_prompt
        best_score = -1.0
        history = []

        # Generate variations of the initial prompt
        variations = self._generate_variations(initial_prompt)
        
        for i, variation in enumerate(variations):
            print(f"\n--- Testing Variation {i+1}/{len(variations)} ---")
            print(f"Prompt: {variation}")
            
            generator.set_prompt_template(variation)
            
            # Evaluate the new prompt on the validation set
            scores = []
            for example in valset:
                generated_output = generator.generate(example)
                eval_inputs = data_mapper.map(generated_output, example)
                
                eval_result = evaluator.evaluate(
                    eval_templates="summary_quality",
                    inputs=eval_inputs,
                    model_name="turing_flash"
                )
                
                if eval_result.eval_results:
                    scores.append(eval_result.eval_results[0].output)

            if not scores:
                continue

            avg_score = sum(scores) / len(scores)
            print(f"Average Score: {avg_score:.4f}")
            
            history.append({"prompt": variation, "score": avg_score})

            if avg_score > best_score:
                best_score = avg_score
                best_prompt = variation
                print("--- New Best Prompt Found! ---")

        # Set the generator to the best prompt found
        generator.set_prompt_template(best_prompt)
        
        return OptimizationResult(
            best_generator=generator,
            history=history,
            final_score=best_score
        )

    def _generate_variations(self, initial_prompt: str) -> List[str]:
        print(f"Generating {self.num_variations} prompt variations with teacher model: {self.teacher_model}...")
        
        instruction = f"""
        You are an expert in prompt engineering. Your task is to generate {self.num_variations} variations of the following prompt.
        The variations should be diverse and explore different styles (e.g., more creative, more concise, more professional, etc.).
        Return ONLY a Python list of the strings, like ["prompt 1", "prompt 2", ...].

        Initial Prompt:
        ---
        {initial_prompt}
        ---
        """
        messages = [{"role": "user", "content": instruction}]
        
        try:
            response = litellm.completion(
                model=self.teacher_model,
                messages=messages,
                **self.teacher_model_kwargs
            )
            
            response_content = response.choices[0].message.content
            # Use eval in a controlled way to parse the string list
            variations = eval(response_content)
            if isinstance(variations, list) and all(isinstance(i, str) for i in variations):
                return variations
            else:
                raise ValueError("Teacher model did not return a valid list of strings.")

        except Exception as e:
            print(f"An error occurred with the teacher model: {e}")
            # Fallback to the placeholder if the teacher model fails
            return [
                initial_prompt + " Be creative.",
                initial_prompt + " Be concise and to the point.",
                initial_prompt + " Write in a professional tone.",
                initial_prompt + " Explain it like I'm five.",
                initial_prompt + " Use a narrative style.",
            ]
