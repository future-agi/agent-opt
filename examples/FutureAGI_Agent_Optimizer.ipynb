{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "32727d13",
      "metadata": {
        "id": "32727d13"
      },
      "source": [
        "# FutureAGI Agent Optimizer — Example Notebook\n",
        "\n",
        "This notebook demonstrates how to use our `agent-opt` library to automatically improve and optimize LLM agents and prompts.\n",
        "It runs a small Question-Answering optimization across multiple optimizers and compares the best prompts found.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6376df67",
      "metadata": {
        "id": "6376df67"
      },
      "outputs": [],
      "source": [
        "# @title Installation\n",
        "# Install dependencies\n",
        "%pip install agent-opt -q\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NxJEZ8PQE8JU",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NxJEZ8PQE8JU",
        "outputId": "7e2dbf21-6881-4a86-f449-02ebdd7be4a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your OPENAI_API_KEY: ··········\n",
            "✅ API Key set successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# Enter your API keys interactively (Jupyter will prompt)\n",
        "OPENAI_API_KEY = getpass.getpass('Enter your OPENAI_API_KEY: ')\n",
        "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY\n",
        "\n",
        "print(\"✅ API Key set successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0dcc117",
      "metadata": {
        "id": "b0dcc117"
      },
      "source": [
        "## Imports and Logging Configuration\n",
        "\n",
        "Import the required modules and configure logging so you can follow optimizer progress.  \n",
        "If an import fails, make sure the corresponding package is installed in your kernel environment.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d95adb4a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d95adb4a",
        "outputId": "f3ce6119-062d-4c51-d8ca-472db76b1cac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All components imported and logging is configured.\n"
          ]
        }
      ],
      "source": [
        "# CELL 2 — Imports and logging configuration\n",
        "import logging\n",
        "import pandas as pd\n",
        "import random\n",
        "import time\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# --- Framework Imports ---\n",
        "from fi.opt.base.base_optimizer import BaseOptimizer\n",
        "from fi.opt.generators import LiteLLMGenerator\n",
        "from fi.opt.datamappers import BasicDataMapper\n",
        "from fi.opt.base.evaluator import Evaluator\n",
        "from fi.opt.types import OptimizationResult, IterationHistory\n",
        "from fi.opt.utils import setup_logging\n",
        "\n",
        "# --- Evaluator Imports ---\n",
        "from fi.evals.metrics import CustomLLMJudge\n",
        "from fi.evals.llm import LiteLLMProvider\n",
        "\n",
        "# --- Import All Optimizers for on the fly changing ---\n",
        "from fi.opt.optimizers import (\n",
        "    RandomSearchOptimizer,\n",
        "    ProTeGi,\n",
        "    MetaPromptOptimizer,\n",
        "    GEPAOptimizer,\n",
        "    PromptWizardOptimizer,\n",
        "    BayesianSearchOptimizer,\n",
        ")\n",
        "\n",
        "# Configure logging\n",
        "setup_logging(level=logging.INFO, log_to_console=True, log_to_file=True, log_file=\"agent-opt.log\")\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "print(\"✅ All components imported and logging is configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e298570f",
      "metadata": {
        "id": "e298570f"
      },
      "source": [
        "## Prepare the Dataset\n",
        "\n",
        "Create an in-memory QA dataset. Replace with your actual dataset for real-life experiment!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6b42715",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6b42715",
        "outputId": "fb32add9-29b2-45ca-b424-897a82ff0aa2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Sample dataset created successfully. Here are the first two examples:\n",
            "{'context': 'The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.', 'question': 'Who designed the Eiffel Tower?', 'answer': 'Gustave Eiffel'}\n",
            "{'context': 'Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy, through a cellular process that converts carbon dioxide and water into glucose and oxygen.', 'question': 'What are the products of photosynthesis?', 'answer': 'Glucose and oxygen'}\n"
          ]
        }
      ],
      "source": [
        "def create_dataset() -> List[Dict[str, Any]]:\n",
        "    '''Creates a sample dataset for the QA task.'''\n",
        "    data = {\n",
        "        'context': [\n",
        "            \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower.\",\n",
        "            \"Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy, through a cellular process that converts carbon dioxide and water into glucose and oxygen.\",\n",
        "            \"The first person to walk on the Moon was Neil Armstrong. The historic event occurred on July 20, 1969, during the Apollo 11 mission.\",\n",
        "            \"The Amazon River in South America is the largest river by discharge volume of water in the world, and the second longest in length.\",\n",
        "            \"William Shakespeare was an English playwright, poet, and actor, widely regarded as the greatest writer in the English language. His plays have been translated into every major living language.\"\n",
        "        ],\n",
        "        'question': [\n",
        "            \"Who designed the Eiffel Tower?\",\n",
        "            \"What are the products of photosynthesis?\",\n",
        "            \"When did a person first walk on the Moon?\",\n",
        "            \"Which river is the largest by water volume?\",\n",
        "            \"What is Shakespeare known for?\"\n",
        "        ],\n",
        "        'answer': [\n",
        "            \"Gustave Eiffel\",\n",
        "            \"Glucose and oxygen\",\n",
        "            \"July 20, 1969\",\n",
        "            \"The Amazon River\",\n",
        "            \"Being the greatest writer in the English language\"\n",
        "        ]\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    return df.to_dict(\"records\")\n",
        "\n",
        "dataset = create_dataset()\n",
        "print(\"✅ Sample dataset created successfully. Here are the first two examples:\")\n",
        "for item in dataset[:2]:\n",
        "    print(item)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d67b2ee",
      "metadata": {
        "id": "1d67b2ee"
      },
      "source": [
        "## Define the Evaluation Strategy\n",
        "\n",
        "We use a Custom LLM-as-a-Judge to score responses from 0.0 to 1.0. You can replace this with a local metric for cost savings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd6f8266",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dd6f8266",
        "outputId": "0d6b3687-1cc0-426d-f440-79fe1d239750"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-02 13:21:36,715 - fi.opt.base.evaluator - INFO - Initialized Evaluator with local metric: CustomLLMJudge\n",
            "✅ Evaluation strategy defined using a Custom LLM-as-a-Judge.\n"
          ]
        }
      ],
      "source": [
        "# LLM provider used by the judge\n",
        "provider = LiteLLMProvider()\n",
        "\n",
        "correctness_judge_config = {\n",
        "    \"name\": \"correctness_judge\",\n",
        "    \"grading_criteria\": '''You are evaluating an AI's answer to a question. The score must be 1.0 if the 'response'\n",
        "is semantically equivalent to the 'expected_response' (the ground truth). The score should be 0.0 if it is incorrect.\n",
        "Partial credit is acceptable. For example, if the expected answer is \"Gustave Eiffel\" and the response is\n",
        "\"The tower was designed by Eiffel\", a score of 0.8 is appropriate.''',\n",
        "}\n",
        "\n",
        "# Instantiate the judge and evaluator wrapper\n",
        "correctness_judge = CustomLLMJudge(provider, config=correctness_judge_config)\n",
        "evaluator = Evaluator(metric=correctness_judge)\n",
        "\n",
        "# Data mapper connects model outputs to the judge expectations\n",
        "data_mapper = BasicDataMapper(\n",
        "    key_map={\n",
        "        \"response\": \"generated_output\",\n",
        "        \"expected_response\": \"answer\"\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"✅ Evaluation strategy defined using a Custom LLM-as-a-Judge.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cd7d895",
      "metadata": {
        "id": "4cd7d895"
      },
      "source": [
        "## Initial Prompt and Models\n",
        "\n",
        "Define the initial prompt and generator model to optimize for, and teacher models to optimize using.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "874202bc",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "874202bc",
        "outputId": "f64a0b22-69a6-4bd1-9058-d1b51590299d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Ready to optimize! We will improve `Context: {context}\\nQuestion: {question}\\nAnswer:`\n"
          ]
        }
      ],
      "source": [
        "INITIAL_PROMPT = \"Context: {context}\\\\nQuestion: {question}\\\\nAnswer:\" # @param {\"type\":\"string\"}\n",
        "GENERATOR_MODEL = \"gpt-4o-mini\" # @param {\"type\":\"string\"}\n",
        "TEACHER_MODEL = \"gpt-5\" # @param {\"type\":\"string\"}\n",
        "\n",
        "print(f\"✅ Ready to optimize! We will improve `{INITIAL_PROMPT}`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bhtoZL7jBxk1",
      "metadata": {
        "id": "bhtoZL7jBxk1"
      },
      "source": [
        "## GEPA Optimizer Setup\n",
        "For this demo, we will be using **GEPA** optimizer.\n",
        "\n",
        " **GEPA** (Genetic-Pareto), is a state-of-the-art evolutionary algorithm for prompt optimization. Instead of making small, random changes, GEPA treats prompts like DNA and intelligently evolves them over generations.\n",
        "\n",
        "### How it Works:\n",
        "1.  **Evaluate:** It first tests the performance of the current best prompt(s) on a sample of data.\n",
        "2.  **Reflect:** It uses a powerful \"reflection\" model (our `reflection_model`) to analyze the results, especially the failures. It generates rich, textual feedback on *why* the prompt failed.\n",
        "3.  **Mutate:** Based on this reflection, it rewrites the prompt to create new, improved \"offspring\" prompts.\n",
        "4.  **Select:** It uses a sophisticated method called Pareto-aware selection to choose the most promising new prompts to carry forward to the next generation. This ensures that it doesn't just find one good prompt, but a diverse set of high-performing ones.\n",
        "\n",
        "This cycle of **Evaluate -> Reflect -> Mutate -> Select** allows GEPA to navigate the vast space of possible prompts much more efficiently than random chance, often leading to significant performance improvements.\n",
        "\n",
        "For more information refer to our [FutureAGI Optimization Docs!](https://docs.futureagi.com/future-agi/get-started/optimization/optimizers/overview)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZLYq-hYJCbk5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZLYq-hYJCbk5",
        "outputId": "344f2e54-af9c-4327-b0f5-d01d220875d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-02 13:22:24,342 - fi.opt.optimizers.gepa - INFO - Initialized with reflection_model: gpt-5, generator_model: gpt-4o-mini\n"
          ]
        }
      ],
      "source": [
        "optimizer = GEPAOptimizer(reflection_model=TEACHER_MODEL, generator_model=GENERATOR_MODEL)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd4c3146",
      "metadata": {
        "id": "cd4c3146"
      },
      "source": [
        "## Run the GEPA Optimizer\n",
        "\n",
        "This cell runs each GEPA Optimizer. It may take time and WILL consume API Credits.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "559ba6df",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "559ba6df",
        "outputId": "928ef513-d82c-4a13-db43-92b9584f7945"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-02 13:35:17,359 - fi.opt.optimizers.gepa - INFO - --- Starting GEPA Prompt Optimization ---\n",
            "2025-10-02 13:35:17,362 - fi.opt.optimizers.gepa - INFO - Dataset size: 5\n",
            "2025-10-02 13:35:17,364 - fi.opt.optimizers.gepa - INFO - Initial prompts: ['Context: {context}\\\\nQuestion: {question}\\\\nAnswer:']\n",
            "2025-10-02 13:35:17,367 - fi.opt.optimizers.gepa - INFO - Max metric calls: 40\n",
            "2025-10-02 13:35:17,368 - fi.opt.optimizers.gepa - INFO - Creating internal GEPA adapter...\n",
            "2025-10-02 13:35:17,368 - fi.opt.optimizers.gepa - INFO - Initialized with generator_model: gpt-4o-mini\n",
            "2025-10-02 13:35:17,369 - fi.opt.optimizers.gepa - INFO - Seed candidate for GEPA: {'prompt': 'Context: {context}\\\\nQuestion: {question}\\\\nAnswer:'}\n",
            "2025-10-02 13:35:17,370 - fi.opt.optimizers.gepa - INFO - Calling gepa.optimize...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "GEPA Optimization:   0%|          | 0/40 [00:00<?, ?rollouts/s]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2025-10-02 13:35:17,378 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:35:17,380 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'Context: {context}\\nQuestion: {question}\\nAnswer:...'\n",
            "2025-10-02 13:35:17,381 - fi.opt.optimizers.gepa - INFO - Batch size: 5\n",
            "2025-10-02 13:35:17,381 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:35:25,646 - fi.opt.optimizers.gepa - INFO - Output generation finished in 8.26s.\n",
            "2025-10-02 13:35:25,647 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:35:25,650 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:35:25,650 - fi.opt.base.evaluator - INFO - Starting evaluation for 5 inputs using 'local' strategy.\n",
            "2025-10-02 13:35:25,651 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:35:31,175 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.8000\n",
            "Reason: {\n",
            "  \"score\": 0.8,\n",
            "  \"reason\": \"The response accurately identifies Gustave Eiffel but provides additional, unnecessary details.\"\n",
            "}\n",
            "2025-10-02 13:35:31,177 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response accurately identifies glucose and oxygen as the products of photosynthesis, matching the expected response.\"\n",
            "}\n",
            "2025-10-02 13:35:31,179 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response includes the correct date, July 20, 1969, but adds extra information not present in the expected response.\"\n",
            "}\n",
            "2025-10-02 13:35:31,179 - fi.opt.base.evaluator - INFO - Input #4 evaluated successfully. Score: 0.5000\n",
            "Reason: {\n",
            "  \"score\": 0.5,\n",
            "  \"reason\": \"The response identifies the Amazon River but includes additional information not required by the expected response.\"\n",
            "}\n",
            "2025-10-02 13:35:31,180 - fi.opt.base.evaluator - INFO - Input #5 evaluated successfully. Score: 0.5000\n",
            "Reason: {\n",
            "  \"score\": 0.5,\n",
            "  \"reason\": \"The response captures the essence of Shakespeare being a great English writer by detailing his contributions and legacy, but it provides more information than required.\"\n",
            "}\n",
            "2025-10-02 13:35:31,181 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 5 results.\n",
            "2025-10-02 13:35:31,182 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 5.53s.\n",
            "2025-10-02 13:35:31,183 - fi.opt.optimizers.gepa - INFO - Scores: [0.8, 1.0, 0.9, 0.5, 0.5]\n",
            "2025-10-02 13:35:31,183 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 13.81s.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "GEPA Optimization:  12%|█▎        | 5/40 [00:13<01:36,  2.76s/rollouts]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0: Base program full valset score: 0.74\n",
            "Iteration 1: Selected program 0 score: 0.74\n",
            "2025-10-02 13:35:31,186 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:35:31,187 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'Context: {context}\\nQuestion: {question}\\nAnswer:...'\n",
            "2025-10-02 13:35:31,188 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:35:31,188 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:35:37,726 - fi.opt.optimizers.gepa - INFO - Output generation finished in 6.54s.\n",
            "2025-10-02 13:35:37,728 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:35:37,729 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:35:37,730 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:35:37,731 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:35:43,572 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.7000\n",
            "Reason: {\n",
            "  \"score\": 0.7,\n",
            "  \"reason\": \"The response provides an extensive description of Shakespeare's achievements, partially addressing the expected response. It highlights his status, works, themes, and contributions, aligning with his recognition as the greatest writer in English. However, it lacks a direct statement matching the succinctness of the expected response.\"\n",
            "}\n",
            "2025-10-02 13:35:43,574 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response includes the correct date, matching the expected response, but contains additional information that was not required.\"\n",
            "}\n",
            "2025-10-02 13:35:43,575 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response accurately identifies the products of photosynthesis, matching the expected response.\"\n",
            "}\n",
            "2025-10-02 13:35:43,576 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:35:43,577 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 5.85s.\n",
            "2025-10-02 13:35:43,578 - fi.opt.optimizers.gepa - INFO - Scores: [0.7, 0.9, 1.0]\n",
            "2025-10-02 13:35:43,579 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-02 13:35:43,580 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 12.39s.\n",
            "2025-10-02 13:35:43,581 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-02 13:35:43,582 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-02 13:35:43,582 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 1: Proposed new text for prompt: You will be given:\n",
            "- Context: a short passage\n",
            "- Question: a query answerable strictly from the Context\n",
            "\n",
            "Your task: Output only the minimal answer needed to correctly answer the Question, using wording from the Context whenever possible.\n",
            "\n",
            "Rules:\n",
            "1) Be concise to the point of a bare phrase or datum. Do not add explanations, examples, extra facts, or restate the question.\n",
            "2) Prefer the exact phrasing that appears in the Context. Avoid synonyms, rewording, or added qualifiers/articles not present in the Context.\n",
            "3) Match the expected form:\n",
            "   - Dates: output only the date in the format used in the Context (e.g., \"July 20, 1969\").\n",
            "   - Names/titles/roles: output only the name/title/role, nothing else.\n",
            "   - Noun phrases: output only the phrase (e.g., \"Being the greatest writer in the English language\").\n",
            "   - Lists: output only the items in the order and conjunctions used in the Context (e.g., \"Glucose and oxygen\").\n",
            "   - Yes/No: answer with \"Yes\" or \"No\" only if the Context makes it explicit.\n",
            "4) Do not add any leading/trailing text, sentences, or qualifiers. No quotes. No bullet points. No trailing punctuation unless it is part of the answer itself.\n",
            "5) Use only information stated in the Context. If the answer is not present or cannot be inferred from the Context, reply exactly: \"Not found in context\".\n",
            "6) Preserve capitalization and spelling as they appear in the Context.\n",
            "\n",
            "Output: A single, standalone answer string that is as short and direct as possible.\n",
            "2025-10-02 13:36:25,514 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:36:25,517 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will be given:\n",
            "- Context: a short passage\n",
            "- Question: a query answerable strictly from the Conte...'\n",
            "2025-10-02 13:36:25,518 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:36:25,519 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:36:27,331 - fi.opt.optimizers.gepa - INFO - Output generation finished in 1.81s.\n",
            "2025-10-02 13:36:27,334 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:36:27,335 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:36:27,336 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:36:27,338 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:36:32,241 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.0000\n",
            "Reason: {\n",
            "  \"score\": 0.0,\n",
            "  \"reason\": \"The response 'Not found in context.' does not convey any part of the expected sentiment or idea of 'Being the greatest writer in the English language.' Thus, a score of 0.0 is justified.\"\n",
            "}\n",
            "2025-10-02 13:36:32,244 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.0000\n",
            "Reason: {\n",
            "  \"score\": 0.0,\n",
            "  \"reason\": \"The response 'Not found in context.' does not provide the expected date 'July 20, 1969', thus it does not meet the criteria for partial credit.\"\n",
            "}\n",
            "2025-10-02 13:36:32,246 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.0000\n",
            "Reason: {\n",
            "  \"score\": 0.0,\n",
            "  \"reason\": \"The response 'Not found in context.' does not provide any of the expected information 'Glucose and oxygen', resulting in a score of 0.0.\"\n",
            "}\n",
            "2025-10-02 13:36:32,247 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:36:32,248 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 4.91s.\n",
            "2025-10-02 13:36:32,249 - fi.opt.optimizers.gepa - INFO - Scores: [0.0, 0.0, 0.0]\n",
            "2025-10-02 13:36:32,250 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 6.74s.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "GEPA Optimization:  28%|██▊       | 11/40 [01:14<03:35,  7.45s/rollouts]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1: New subsample score 0.0 is not better than old score 2.6, skipping\n",
            "Iteration 2: Selected program 0 score: 0.74\n",
            "2025-10-02 13:36:32,255 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:36:32,256 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'Context: {context}\\nQuestion: {question}\\nAnswer:...'\n",
            "2025-10-02 13:36:32,257 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:36:32,257 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:36:37,702 - fi.opt.optimizers.gepa - INFO - Output generation finished in 5.44s.\n",
            "2025-10-02 13:36:37,705 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:36:37,706 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:36:37,707 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:36:37,708 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:36:42,490 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.7000\n",
            "Reason: {\n",
            "  \"score\": 0.7,\n",
            "  \"reason\": \"The response correctly identifies Gustave Eiffel as the designer but includes additional and unnecessary details about his company. The inclusion of 'his company' slightly deviates from the expected concise answer.\"\n",
            "}\n",
            "2025-10-02 13:36:42,492 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.5000\n",
            "Reason: {\n",
            "  \"score\": 0.5,\n",
            "  \"reason\": \"The response correctly identifies the Amazon River but includes additional information that is irrelevant to the expected response.\"\n",
            "}\n",
            "2025-10-02 13:36:42,493 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.4000\n",
            "Reason: {\n",
            "  \"score\": 0.4,\n",
            "  \"reason\": \"The response provides some relevant information by identifying 'The Amazon River,' but it includes additional context ('is the largest river by discharge volume of water in the world') that was not required, indicating a partial match.\"\n",
            "}\n",
            "2025-10-02 13:36:42,494 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:36:42,496 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 4.79s.\n",
            "2025-10-02 13:36:42,497 - fi.opt.optimizers.gepa - INFO - Scores: [0.7, 0.5, 0.4]\n",
            "2025-10-02 13:36:42,498 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-02 13:36:42,499 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 10.24s.\n",
            "2025-10-02 13:36:42,500 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-02 13:36:42,501 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-02 13:36:42,502 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 2: Proposed new text for prompt: You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task: Provide only the minimal, exact answer to the question based solely on the provided context.\n",
            "\n",
            "Strict output requirements:\n",
            "- Output only the answer text (a single short phrase or name). Do not restate the question or context.\n",
            "- Do not write a full sentence. No extra words, qualifiers, or explanations.\n",
            "- Do not add punctuation (e.g., no trailing period).\n",
            "- Prefer an exact phrase copied from the context when possible.\n",
            "- Use the canonical capitalization and wording as it appears in the context. Include definite articles only if they are part of the entity’s name in the context (e.g., “The Amazon River”).\n",
            "- Provide the shortest unambiguous span that fully answers the question. Do not add related entities or attributions (e.g., avoid “and his company”).\n",
            "- No quotes, no formatting, no prefixes/suffixes.\n",
            "\n",
            "Guidance:\n",
            "- This is extractive QA: identify and return the minimal answer span from the context that directly answers the question.\n",
            "- If multiple mentions could answer, choose the shortest one that is fully correct and unambiguous as written in the context.\n",
            "2025-10-02 13:37:20,937 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:37:20,940 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task: Provid...'\n",
            "2025-10-02 13:37:20,941 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:37:20,943 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:37:22,609 - fi.opt.optimizers.gepa - INFO - Output generation finished in 1.67s.\n",
            "2025-10-02 13:37:22,611 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:37:22,612 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:37:22,614 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:37:22,616 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:37:26,977 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is identical to the expected response, thus it is semantically equivalent.\"\n",
            "}\n",
            "2025-10-02 13:37:26,979 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response correctly identifies the Amazon River, but does not match the capitalization of the expected response, which slightly affects the grading.\"\n",
            "}\n",
            "2025-10-02 13:37:26,982 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.7000\n",
            "Reason: {\n",
            "  \"score\": 0.7,\n",
            "  \"reason\": \"The response correctly describes the Amazon River and clearly shows an understanding of its significance by discharge volume, but it does not mention the name 'The Amazon River' explicitly, resulting in partial credit.\"\n",
            "}\n",
            "2025-10-02 13:37:26,983 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:37:26,984 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 4.37s.\n",
            "2025-10-02 13:37:26,985 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 0.9, 0.7]\n",
            "2025-10-02 13:37:26,988 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 6.05s.\n",
            "Iteration 2: New subsample score 2.6 is better than old score 1.6. Continue to full eval and add to candidate pool.\n",
            "2025-10-02 13:37:26,989 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:37:26,991 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task: Provid...'\n",
            "2025-10-02 13:37:26,992 - fi.opt.optimizers.gepa - INFO - Batch size: 5\n",
            "2025-10-02 13:37:26,994 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:37:29,949 - fi.opt.optimizers.gepa - INFO - Output generation finished in 2.95s.\n",
            "2025-10-02 13:37:29,951 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:37:29,952 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:37:29,953 - fi.opt.base.evaluator - INFO - Starting evaluation for 5 inputs using 'local' strategy.\n",
            "2025-10-02 13:37:29,955 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:37:36,204 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-02 13:37:36,206 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response is semantically correct and closely matches the expected response, differing only in capitalization.\"\n",
            "}\n",
            "2025-10-02 13:37:36,207 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is exactly the same as the expected response, matching both date and format.\"\n",
            "}\n",
            "2025-10-02 13:37:36,209 - fi.opt.base.evaluator - INFO - Input #4 evaluated successfully. Score: 0.2000\n",
            "Reason: {\n",
            "  \"score\": 0.2,\n",
            "  \"reason\": \"The response references the correct characteristic of the Amazon River, being the largest river by discharge volume, but does not explicitly mention \\\"The Amazon River.\\\" Partial credit is given for identifying the key feature.\"\n",
            "}\n",
            "2025-10-02 13:37:36,210 - fi.opt.base.evaluator - INFO - Input #5 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response is semantically very close to the expected answer. Both describe the same idea, with minor differences in phrasing.\"\n",
            "}\n",
            "2025-10-02 13:37:36,211 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 5 results.\n",
            "2025-10-02 13:37:36,211 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 6.26s.\n",
            "2025-10-02 13:37:36,212 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 0.9, 1.0, 0.2, 0.9]\n",
            "2025-10-02 13:37:36,213 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 9.22s.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "GEPA Optimization:  55%|█████▌    | 22/40 [02:18<01:55,  6.43s/rollouts]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 2: New program is on the linear pareto front\n",
            "Iteration 2: Full valset score for new program: 0.8\n",
            "Iteration 2: Full train_val score for new program: 0.8\n",
            "Iteration 2: Individual valset scores for new program: [1.0, 0.9, 1.0, 0.2, 0.9]\n",
            "Iteration 2: New valset pareto front scores: [1.0, 1.0, 1.0, 0.5, 0.9]\n",
            "Iteration 2: Full valset pareto front score: 0.8800000000000001\n",
            "Iteration 2: Updated valset pareto front programs: [{1}, {0}, {1}, {0}, {1}]\n",
            "Iteration 2: Best valset aggregate score so far: 0.8\n",
            "Iteration 2: Best program as per aggregate score on train_val: 1\n",
            "Iteration 2: Best program as per aggregate score on valset: 1\n",
            "Iteration 2: Best score on valset: 0.8\n",
            "Iteration 2: Best score on train_val: 0.8\n",
            "Iteration 2: Linear pareto front program index: 1\n",
            "Iteration 2: New program candidate index: 1\n",
            "Iteration 3: Selected program 1 score: 0.8\n",
            "2025-10-02 13:37:36,216 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:37:36,217 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task: Provid...'\n",
            "2025-10-02 13:37:36,218 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:37:36,218 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:37:38,465 - fi.opt.optimizers.gepa - INFO - Output generation finished in 2.25s.\n",
            "2025-10-02 13:37:38,468 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:37:38,468 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:37:38,469 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:37:38,470 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:37:41,092 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, differing only in capitalization, which is not significant in this context.\"\n",
            "}\n",
            "2025-10-02 13:37:41,095 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-02 13:37:41,096 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.8000\n",
            "Reason: {\n",
            "  \"score\": 0.8,\n",
            "  \"reason\": \"The response conveys the core idea of the expected response, missing only the introductory words 'Being the' at the start.\"\n",
            "}\n",
            "2025-10-02 13:37:41,096 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:37:41,097 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 2.63s.\n",
            "2025-10-02 13:37:41,099 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 1.0, 0.8]\n",
            "2025-10-02 13:37:41,100 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-02 13:37:41,101 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 4.88s.\n",
            "2025-10-02 13:37:41,102 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-02 13:37:41,104 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-02 13:37:41,104 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 3: Proposed new text for prompt: Task: Return the minimal, exact answer to the question using only the provided context.\n",
            "\n",
            "Output format (strict):\n",
            "- Output only the answer text (a single short phrase/name). One line only.\n",
            "- No restating the question or context.\n",
            "- No full sentences, no explanations, no qualifiers.\n",
            "- No quotes or surrounding characters. No terminal punctuation.\n",
            "- Include punctuation only if it is integral to the entity (e.g., hyphens/apostrophes in names).\n",
            "- Use the exact wording and capitalization as it appears in the context whenever possible.\n",
            "- Include definite articles only if they are part of the entity’s canonical name in the context (e.g., “The Amazon River”).\n",
            "- Provide the shortest unambiguous span that fully answers the question.\n",
            "- Do not add related entities or appositives (avoid “and his company,” “the engineer,” etc.).\n",
            "- Preserve the order and conjunctions as in the context for list answers (e.g., “glucose and oxygen”).\n",
            "- No leading/trailing spaces or line breaks.\n",
            "\n",
            "Selection rules:\n",
            "- This is extractive QA: identify and return a contiguous phrase from the context that directly answers the question.\n",
            "- If multiple spans could answer, choose the shortest one that is fully correct and unambiguous.\n",
            "- Match the entity type asked (who/what/when/where). Prefer a person name over their organization if the question asks “Who” and the context attributes the action via that person’s company/team.\n",
            "- For numbers/dates/units, copy exactly as written in the context.\n",
            "- Do not paraphrase or infer beyond the context, and do not include words not present in the selected span.\n",
            "\n",
            "Quality checks before submitting:\n",
            "- Is the span fully contained in the context and minimally sufficient?\n",
            "- Does it use the context’s wording and casing?\n",
            "- Does it avoid extra words, punctuation, and formatting?\n",
            "2025-10-02 13:38:42,300 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:38:42,302 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'Task: Return the minimal, exact answer to the question using only the provided context.\n",
            "\n",
            "Output form...'\n",
            "2025-10-02 13:38:42,306 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:38:42,307 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:38:44,332 - fi.opt.optimizers.gepa - INFO - Output generation finished in 2.02s.\n",
            "2025-10-02 13:38:44,335 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:38:44,336 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:38:44,337 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:38:44,338 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:38:46,764 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response matches the expected response exactly, indicating semantic equivalence.\"\n",
            "}\n",
            "2025-10-02 13:38:46,767 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.0000\n",
            "Reason: {\n",
            "  \"score\": 0.0,\n",
            "  \"reason\": \"The response is unrelated to the expected answer. 'glucose and oxygen' does not match 'Gustave Eiffel' in any aspect.\"\n",
            "}\n",
            "2025-10-02 13:38:46,768 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.0000\n",
            "Reason: {\n",
            "  \"score\": 0.0,\n",
            "  \"reason\": \"The response is completely incorrect and irrelevant to the expected response.\"\n",
            "}\n",
            "2025-10-02 13:38:46,770 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:38:46,771 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 2.43s.\n",
            "2025-10-02 13:38:46,772 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 0.0, 0.0]\n",
            "2025-10-02 13:38:46,774 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 4.47s.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "GEPA Optimization:  70%|███████   | 28/40 [03:29<01:38,  8.17s/rollouts]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 3: New subsample score 1.0 is not better than old score 2.8, skipping\n",
            "Iteration 4: Selected program 0 score: 0.74\n",
            "2025-10-02 13:38:46,783 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:38:46,784 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'Context: {context}\\nQuestion: {question}\\nAnswer:...'\n",
            "2025-10-02 13:38:46,785 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:38:46,787 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:38:48,908 - fi.opt.optimizers.gepa - INFO - Output generation finished in 2.12s.\n",
            "2025-10-02 13:38:48,912 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:38:48,914 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:38:48,914 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:38:48,915 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:38:52,947 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response includes the correct date with additional context, which is not necessary but does not alter the accuracy of the date itself.\"\n",
            "}\n",
            "2025-10-02 13:38:52,949 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.6000\n",
            "Reason: {\n",
            "  \"score\": 0.6,\n",
            "  \"reason\": \"The response correctly identifies that the Amazon River is significant but includes additional information about its discharge volume, which is not needed. While the core identification was correct, the extra detail slightly reduces its directness to the expected response.\"\n",
            "}\n",
            "2025-10-02 13:38:52,950 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response correctly identifies Gustave Eiffel as the designer, but includes additional information about his company.\"\n",
            "}\n",
            "2025-10-02 13:38:52,951 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:38:52,953 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 4.04s.\n",
            "2025-10-02 13:38:52,953 - fi.opt.optimizers.gepa - INFO - Scores: [0.9, 0.6, 0.9]\n",
            "2025-10-02 13:38:52,954 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-02 13:38:52,955 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 6.17s.\n",
            "2025-10-02 13:38:52,956 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-02 13:38:52,957 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-02 13:38:52,958 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 4: Proposed new text for prompt: You will be given input in this format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:\n",
            "Provide the shortest, direct answer to the question using only information from the Context. When possible, copy the exact minimal phrase from the Context that answers the question.\n",
            "\n",
            "Strict output rules:\n",
            "- Output only the answer text on a single line after “Answer:”.\n",
            "- Do not add any extra words, sentences, explanations, or restatements.\n",
            "- Do not include prefixes like “The answer is:”.\n",
            "- Do not add punctuation at the end (e.g., no trailing period) unless it is part of the answer itself.\n",
            "- Preserve capitalization as it appears in the Context.\n",
            "- Prefer an exact extractive span from the Context; avoid paraphrasing.\n",
            "\n",
            "Guidance by question type:\n",
            "- “When” questions: output the date exactly as written in the Context (e.g., “July 20, 1969”).\n",
            "- “Who” questions: output the person’s name only (e.g., “Gustave Eiffel”).\n",
            "- “Which/What” entity questions: output the entity name only (e.g., “The Amazon River”).\n",
            "\n",
            "Important:\n",
            "- Do not include any additional contextual details (even if true), such as descriptions or qualifications (e.g., avoid appending “by discharge volume of water in the world”).\n",
            "- Use only the provided Context; do not rely on external knowledge.\n",
            "- If the answer is not present in the Context, respond with: Unknown\n",
            "2025-10-02 13:39:41,039 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:39:41,041 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will be given input in this format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:\n",
            "Provi...'\n",
            "2025-10-02 13:39:41,042 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:39:41,043 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:39:44,668 - fi.opt.optimizers.gepa - INFO - Output generation finished in 3.62s.\n",
            "2025-10-02 13:39:44,671 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:39:44,673 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:39:44,674 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:39:44,675 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:39:47,601 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, only containing the additional word 'Answer', which does not change the meaning.\"\n",
            "}\n",
            "2025-10-02 13:39:47,604 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-02 13:39:47,605 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response includes the expected answer and is mostly correct but adds unnecessary words ('Answer:'). Thus, it is slightly less concise.\"\n",
            "}\n",
            "2025-10-02 13:39:47,606 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:39:47,607 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 2.93s.\n",
            "2025-10-02 13:39:47,608 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 1.0, 0.9]\n",
            "2025-10-02 13:39:47,609 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 6.57s.\n",
            "Iteration 4: New subsample score 2.9 is better than old score 2.4. Continue to full eval and add to candidate pool.\n",
            "2025-10-02 13:39:47,609 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:39:47,610 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will be given input in this format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:\n",
            "Provi...'\n",
            "2025-10-02 13:39:47,611 - fi.opt.optimizers.gepa - INFO - Batch size: 5\n",
            "2025-10-02 13:39:47,612 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:39:51,766 - fi.opt.optimizers.gepa - INFO - Output generation finished in 4.15s.\n",
            "2025-10-02 13:39:51,769 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:39:51,770 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:39:51,771 - fi.opt.base.evaluator - INFO - Starting evaluation for 5 inputs using 'local' strategy.\n",
            "2025-10-02 13:39:51,772 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:39:57,385 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-02 13:39:57,388 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 0.9500\n",
            "Reason: {\n",
            "  \"score\": 0.95,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response but includes an unnecessary prefix 'Answer:' which slightly affects conciseness.\"\n",
            "}\n",
            "2025-10-02 13:39:57,391 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response accurately replicates the date in the expected response with only minor additional framing that does not affect the core correctness.\"\n",
            "}\n",
            "2025-10-02 13:39:57,392 - fi.opt.base.evaluator - INFO - Input #4 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, providing the same information.\"\n",
            "}\n",
            "2025-10-02 13:39:57,392 - fi.opt.base.evaluator - INFO - Input #5 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response and the expected response are semantically very similar. Both convey the concept of being regarded as the greatest writer in the English language, with minor differences in wording.\"\n",
            "}\n",
            "2025-10-02 13:39:57,393 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 5 results.\n",
            "2025-10-02 13:39:57,395 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 5.62s.\n",
            "2025-10-02 13:39:57,396 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 0.95, 1.0, 1.0, 0.9]\n",
            "2025-10-02 13:39:57,398 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 9.79s.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "GEPA Optimization:  98%|█████████▊| 39/40 [04:40<00:07,  7.36s/rollouts]\u001b[A"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 4: New program is on the linear pareto front\n",
            "Iteration 4: Full valset score for new program: 0.97\n",
            "Iteration 4: Full train_val score for new program: 0.97\n",
            "Iteration 4: Individual valset scores for new program: [1.0, 0.95, 1.0, 1.0, 0.9]\n",
            "Iteration 4: New valset pareto front scores: [1.0, 1.0, 1.0, 1.0, 0.9]\n",
            "Iteration 4: Full valset pareto front score: 0.9800000000000001\n",
            "Iteration 4: Updated valset pareto front programs: [{1, 2}, {0}, {1, 2}, {2}, {1, 2}]\n",
            "Iteration 4: Best valset aggregate score so far: 0.97\n",
            "Iteration 4: Best program as per aggregate score on train_val: 2\n",
            "Iteration 4: Best program as per aggregate score on valset: 2\n",
            "Iteration 4: Best score on valset: 0.97\n",
            "Iteration 4: Best score on train_val: 0.97\n",
            "Iteration 4: Linear pareto front program index: 2\n",
            "Iteration 4: New program candidate index: 2\n",
            "Iteration 5: Selected program 2 score: 0.97\n",
            "2025-10-02 13:39:57,405 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:39:57,406 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will be given input in this format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:\n",
            "Provi...'\n",
            "2025-10-02 13:39:57,407 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:39:57,410 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:39:59,589 - fi.opt.optimizers.gepa - INFO - Output generation finished in 2.18s.\n",
            "2025-10-02 13:39:59,592 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:39:59,593 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:39:59,594 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:39:59,596 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:40:03,424 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.8000\n",
            "Reason: {\n",
            "  \"score\": 0.8,\n",
            "  \"reason\": \"The response conveys the main idea of the expected response, but lacks the context of 'being' which implies a role or status.\"\n",
            "}\n",
            "2025-10-02 13:40:03,428 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-02 13:40:03,429 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.9000\n",
            "Reason: {\n",
            "  \"score\": 0.9,\n",
            "  \"reason\": \"The response includes the expected answer exactly, but with additional unnecessary text ('Answer:'). The core answer is correct.\"\n",
            "}\n",
            "2025-10-02 13:40:03,430 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:40:03,431 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 3.84s.\n",
            "2025-10-02 13:40:03,432 - fi.opt.optimizers.gepa - INFO - Scores: [0.8, 1.0, 0.9]\n",
            "2025-10-02 13:40:03,433 - fi.opt.optimizers.gepa - INFO - Capturing traces.\n",
            "2025-10-02 13:40:03,434 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 6.03s.\n",
            "2025-10-02 13:40:03,435 - fi.opt.optimizers.gepa - INFO - Creating reflective dataset.\n",
            "2025-10-02 13:40:03,436 - fi.opt.optimizers.gepa - INFO - Processing 3 trajectories.\n",
            "2025-10-02 13:40:03,437 - fi.opt.optimizers.gepa - INFO - Reflective dataset created for components: ['prompt']\n",
            "Iteration 5: Proposed new text for prompt: You will receive inputs in this exact format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:\n",
            "Return the shortest, direct answer to the Question using only information from the Context. Prefer copying the exact minimal phrase from the Context.\n",
            "\n",
            "Strict output rules:\n",
            "- Output exactly one line in the form: Answer: {answer}\n",
            "- Do not add any extra words, sentences, explanations, or restatements.\n",
            "- Do not include prefixes like “The answer is”.\n",
            "- Do not add punctuation at the end unless it is part of the answer itself.\n",
            "- Preserve capitalization exactly as it appears in the Context.\n",
            "- Use only the provided Context; do not rely on external knowledge.\n",
            "- If the answer is not present or cannot be determined from the Context, output: Answer: Unknown\n",
            "\n",
            "Answer selection rules:\n",
            "- Prefer an exact extractive span from the Context; avoid paraphrasing.\n",
            "- Choose the shortest span that fully answers the question.\n",
            "- Keep the span contiguous when possible. You may drop surrounding hedges or qualifiers that are not needed for the direct answer (e.g., remove “widely regarded as” if not essential).\n",
            "- Do not introduce new content or change meaning.\n",
            "\n",
            "Allowed minimal adjustment (only when necessary for grammaticality):\n",
            "- For “known for/role/status” questions (e.g., “What is X known for?”), you may prepend “Being” to an otherwise extractive noun phrase to form a grammatical fragment (e.g., “Being the …”). Do not add any other words.\n",
            "- No other paraphrasing or additions are allowed.\n",
            "\n",
            "Guidance by question type:\n",
            "- When questions: output the date exactly as written in the Context (e.g., “July 20, 1969”).\n",
            "- Who questions: output the person’s name only, exactly as in the Context (include initials or middle names if present).\n",
            "- Which/What entity questions: output the entity name only, including any leading article present in the Context (e.g., “The Amazon River”).\n",
            "- Where questions: output the location phrase only, exactly as written.\n",
            "\n",
            "Formatting reminders:\n",
            "- Single line only.\n",
            "- Exactly one “Answer:” prefix.\n",
            "- No trailing punctuation unless it appears in the Context’s answer span.\n",
            "2025-10-02 13:41:04,193 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:41:04,197 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will receive inputs in this exact format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:...'\n",
            "2025-10-02 13:41:04,198 - fi.opt.optimizers.gepa - INFO - Batch size: 3\n",
            "2025-10-02 13:41:04,199 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:41:08,367 - fi.opt.optimizers.gepa - INFO - Output generation finished in 4.17s.\n",
            "2025-10-02 13:41:08,369 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:41:08,370 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:41:08,371 - fi.opt.base.evaluator - INFO - Starting evaluation for 3 inputs using 'local' strategy.\n",
            "2025-10-02 13:41:08,371 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:41:12,502 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response with only minor formatting difference (prefix 'Answer:'). Therefore, it fully meets the expected result.\"\n",
            "}\n",
            "2025-10-02 13:41:12,504 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response. Both identify 'The Amazon River' as the answer.\"\n",
            "}\n",
            "2025-10-02 13:41:12,505 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 0.9500\n",
            "Reason: {\n",
            "  \"score\": 0.95,\n",
            "  \"reason\": \"The response is semantically equivalent but includes an additional word ('Answer:'). However, this only slightly detracts from the precision of the expected response.\"\n",
            "}\n",
            "2025-10-02 13:41:12,506 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 3 results.\n",
            "2025-10-02 13:41:12,507 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 4.14s.\n",
            "2025-10-02 13:41:12,508 - fi.opt.optimizers.gepa - INFO - Scores: [1.0, 1.0, 0.95]\n",
            "2025-10-02 13:41:12,509 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 8.32s.\n",
            "Iteration 5: New subsample score 2.95 is better than old score 2.7. Continue to full eval and add to candidate pool.\n",
            "2025-10-02 13:41:12,511 - fi.opt.optimizers.gepa - INFO - Starting evaluation for a candidate prompt.\n",
            "2025-10-02 13:41:12,512 - fi.opt.optimizers.gepa - INFO - Evaluating prompt: 'You will receive inputs in this exact format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:...'\n",
            "2025-10-02 13:41:12,513 - fi.opt.optimizers.gepa - INFO - Batch size: 5\n",
            "2025-10-02 13:41:12,515 - fi.opt.optimizers.gepa - INFO - Generating outputs...\n",
            "2025-10-02 13:41:18,238 - fi.opt.optimizers.gepa - INFO - Output generation finished in 5.72s.\n",
            "2025-10-02 13:41:18,242 - fi.opt.optimizers.gepa - INFO - Mapping evaluation inputs...\n",
            "2025-10-02 13:41:18,243 - fi.opt.optimizers.gepa - INFO - Evaluating generated outputs...\n",
            "2025-10-02 13:41:18,245 - fi.opt.base.evaluator - INFO - Starting evaluation for 5 inputs using 'local' strategy.\n",
            "2025-10-02 13:41:18,245 - fi.opt.base.evaluator - INFO - Running local evaluation with metric: CustomLLMJudge\n",
            "2025-10-02 13:41:24,231 - fi.opt.base.evaluator - INFO - Input #1 evaluated successfully. Score: 0.9500\n",
            "Reason: {\n",
            "  \"score\": 0.95,\n",
            "  \"reason\": \"The response provides the correct answer in a clear and explicit form, 'Answer: Gustave Eiffel'. This matches the expected response semantically, but the format is slightly longer than necessary, which might introduce minor noise compared to the expected answer.\"\n",
            "}\n",
            "2025-10-02 13:41:24,234 - fi.opt.base.evaluator - INFO - Input #2 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, merely adding 'Answer:' at the beginning.\"\n",
            "}\n",
            "2025-10-02 13:41:24,237 - fi.opt.base.evaluator - INFO - Input #3 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response matches the expected response exactly, just with the addition of the word 'Answer:' which does not change the semantic meaning.\"\n",
            "}\n",
            "2025-10-02 13:41:24,239 - fi.opt.base.evaluator - INFO - Input #4 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response, correctly identifying 'The Amazon River'.\"\n",
            "}\n",
            "2025-10-02 13:41:24,241 - fi.opt.base.evaluator - INFO - Input #5 evaluated successfully. Score: 1.0000\n",
            "Reason: {\n",
            "  \"score\": 1.0,\n",
            "  \"reason\": \"The response is semantically equivalent to the expected response.\"\n",
            "}\n",
            "2025-10-02 13:41:24,242 - fi.opt.base.evaluator - INFO - Local evaluation completed. Returning 5 results.\n",
            "2025-10-02 13:41:24,245 - fi.opt.optimizers.gepa - INFO - Evaluation with framework evaluator finished in 6.00s.\n",
            "2025-10-02 13:41:24,246 - fi.opt.optimizers.gepa - INFO - Scores: [0.95, 1.0, 1.0, 1.0, 1.0]\n",
            "2025-10-02 13:41:24,248 - fi.opt.optimizers.gepa - INFO - Evaluation finished in 11.74s.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rGEPA Optimization:  98%|█████████▊| 39/40 [06:06<00:09,  9.41s/rollouts]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 5: New program is on the linear pareto front\n",
            "Iteration 5: Full valset score for new program: 0.99\n",
            "Iteration 5: Full train_val score for new program: 0.99\n",
            "Iteration 5: Individual valset scores for new program: [0.95, 1.0, 1.0, 1.0, 1.0]\n",
            "Iteration 5: New valset pareto front scores: [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "Iteration 5: Full valset pareto front score: 1.0\n",
            "Iteration 5: Updated valset pareto front programs: [{1, 2}, {0, 3}, {1, 2, 3}, {2, 3}, {3}]\n",
            "Iteration 5: Best valset aggregate score so far: 0.99\n",
            "Iteration 5: Best program as per aggregate score on train_val: 3\n",
            "Iteration 5: Best program as per aggregate score on valset: 3\n",
            "Iteration 5: Best score on valset: 0.99\n",
            "Iteration 5: Best score on train_val: 0.99\n",
            "Iteration 5: Linear pareto front program index: 3\n",
            "Iteration 5: New program candidate index: 3\n",
            "2025-10-02 13:41:24,251 - fi.opt.optimizers.gepa - INFO - gepa.optimize finished in 366.88s.\n",
            "2025-10-02 13:41:24,253 - fi.opt.optimizers.gepa - INFO - GEPA result best score: 0.99\n",
            "2025-10-02 13:41:24,255 - fi.opt.optimizers.gepa - INFO - GEPA best candidate: {'prompt': 'You will receive inputs in this exact format:\\nContext: {context}\\nQuestion: {question}\\nAnswer:\\n\\nTask:\\nReturn the shortest, direct answer to the Question using only information from the Context. Prefer copying the exact minimal phrase from the Context.\\n\\nStrict output rules:\\n- Output exactly one line in the form: Answer: {answer}\\n- Do not add any extra words, sentences, explanations, or restatements.\\n- Do not include prefixes like “The answer is”.\\n- Do not add punctuation at the end unless it is part of the answer itself.\\n- Preserve capitalization exactly as it appears in the Context.\\n- Use only the provided Context; do not rely on external knowledge.\\n- If the answer is not present or cannot be determined from the Context, output: Answer: Unknown\\n\\nAnswer selection rules:\\n- Prefer an exact extractive span from the Context; avoid paraphrasing.\\n- Choose the shortest span that fully answers the question.\\n- Keep the span contiguous when possible. You may drop surrounding hedges or qualifiers that are not needed for the direct answer (e.g., remove “widely regarded as” if not essential).\\n- Do not introduce new content or change meaning.\\n\\nAllowed minimal adjustment (only when necessary for grammaticality):\\n- For “known for/role/status” questions (e.g., “What is X known for?”), you may prepend “Being” to an otherwise extractive noun phrase to form a grammatical fragment (e.g., “Being the …”). Do not add any other words.\\n- No other paraphrasing or additions are allowed.\\n\\nGuidance by question type:\\n- When questions: output the date exactly as written in the Context (e.g., “July 20, 1969”).\\n- Who questions: output the person’s name only, exactly as in the Context (include initials or middle names if present).\\n- Which/What entity questions: output the entity name only, including any leading article present in the Context (e.g., “The Amazon River”).\\n- Where questions: output the location phrase only, exactly as written.\\n\\nFormatting reminders:\\n- Single line only.\\n- Exactly one “Answer:” prefix.\\n- No trailing punctuation unless it appears in the Context’s answer span.'}\n",
            "2025-10-02 13:41:24,256 - fi.opt.optimizers.gepa - INFO - Translating GEPA result to OptimizationResult...\n",
            "2025-10-02 13:41:24,258 - fi.opt.optimizers.gepa - INFO - --- GEPA Prompt Optimization finished in 366.90s ---\n",
            "2025-10-02 13:41:24,261 - fi.opt.optimizers.gepa - INFO - Final best score: 0.99\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "results = optimizer.optimize(\n",
        "    evaluator = evaluator,\n",
        "    data_mapper = data_mapper,\n",
        "    dataset = dataset,\n",
        "    initial_prompts = [INITIAL_PROMPT],\n",
        "    max_metric_calls = 40 # Since our dataset is small and isn't too complex, a lower limit should suffice.\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d737702",
      "metadata": {
        "id": "1d737702"
      },
      "source": [
        "## Final Results of GEPA Optimizer Run\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34df1c40",
      "metadata": {},
      "source": [
        "### Best Prompt Found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dJLUazEYKMae",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dJLUazEYKMae",
        "outputId": "2cc2bc55-6fbd-4d6d-c552-6531c30cdfc9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You will receive inputs in this exact format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:\n",
            "Return the shortest, direct answer to the Question using only information from the Context. Prefer copying the exact minimal phrase from the Context.\n",
            "\n",
            "Strict output rules:\n",
            "- Output exactly one line in the form: Answer: {answer}\n",
            "- Do not add any extra words, sentences, explanations, or restatements.\n",
            "- Do not include prefixes like “The answer is”.\n",
            "- Do not add punctuation at the end unless it is part of the answer itself.\n",
            "- Preserve capitalization exactly as it appears in the Context.\n",
            "- Use only the provided Context; do not rely on external knowledge.\n",
            "- If the answer is not present or cannot be determined from the Context, output: Answer: Unknown\n",
            "\n",
            "Answer selection rules:\n",
            "- Prefer an exact extractive span from the Context; avoid paraphrasing.\n",
            "- Choose the shortest span that fully answers the question.\n",
            "- Keep the span contiguous when possible. You may drop surrounding hedges or qualifiers that are not needed for the direct answer (e.g., remove “widely regarded as” if not essential).\n",
            "- Do not introduce new content or change meaning.\n",
            "\n",
            "Allowed minimal adjustment (only when necessary for grammaticality):\n",
            "- For “known for/role/status” questions (e.g., “What is X known for?”), you may prepend “Being” to an otherwise extractive noun phrase to form a grammatical fragment (e.g., “Being the …”). Do not add any other words.\n",
            "- No other paraphrasing or additions are allowed.\n",
            "\n",
            "Guidance by question type:\n",
            "- When questions: output the date exactly as written in the Context (e.g., “July 20, 1969”).\n",
            "- Who questions: output the person’s name only, exactly as in the Context (include initials or middle names if present).\n",
            "- Which/What entity questions: output the entity name only, including any leading article present in the Context (e.g., “The Amazon River”).\n",
            "- Where questions: output the location phrase only, exactly as written.\n",
            "\n",
            "Formatting reminders:\n",
            "- Single line only.\n",
            "- Exactly one “Answer:” prefix.\n",
            "- No trailing punctuation unless it appears in the Context’s answer span.\n"
          ]
        }
      ],
      "source": [
        "print(f\"{results.best_generator.get_prompt_template()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d27b16f",
      "metadata": {},
      "source": [
        "### Final Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fXM2knnxKyjD",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fXM2knnxKyjD",
        "outputId": "e5c2a7bc-5128-4df8-abb5-0843929d1aa2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.99"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "results.final_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a61d3d1b",
      "metadata": {},
      "source": [
        "### Iteration History"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XreFo1r6LEiN",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XreFo1r6LEiN",
        "outputId": "d6310f87-fbea-463d-da7c-246c76f55be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---- Iteration 1 ----\n",
            "===PROMPT===\n",
            "Context: {context}\\nQuestion: {question}\\nAnswer:\n",
            "\n",
            "\n",
            "===AVERAGE SCORE===\n",
            "0.74\n",
            "---- Iteration 2 ----\n",
            "===PROMPT===\n",
            "You are given inputs in the form:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Your task: Provide only the minimal, exact answer to the question based solely on the provided context.\n",
            "\n",
            "Strict output requirements:\n",
            "- Output only the answer text (a single short phrase or name). Do not restate the question or context.\n",
            "- Do not write a full sentence. No extra words, qualifiers, or explanations.\n",
            "- Do not add punctuation (e.g., no trailing period).\n",
            "- Prefer an exact phrase copied from the context when possible.\n",
            "- Use the canonical capitalization and wording as it appears in the context. Include definite articles only if they are part of the entity’s name in the context (e.g., “The Amazon River”).\n",
            "- Provide the shortest unambiguous span that fully answers the question. Do not add related entities or attributions (e.g., avoid “and his company”).\n",
            "- No quotes, no formatting, no prefixes/suffixes.\n",
            "\n",
            "Guidance:\n",
            "- This is extractive QA: identify and return the minimal answer span from the context that directly answers the question.\n",
            "- If multiple mentions could answer, choose the shortest one that is fully correct and unambiguous as written in the context.\n",
            "\n",
            "\n",
            "===AVERAGE SCORE===\n",
            "0.8\n",
            "---- Iteration 3 ----\n",
            "===PROMPT===\n",
            "You will be given input in this format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:\n",
            "Provide the shortest, direct answer to the question using only information from the Context. When possible, copy the exact minimal phrase from the Context that answers the question.\n",
            "\n",
            "Strict output rules:\n",
            "- Output only the answer text on a single line after “Answer:”.\n",
            "- Do not add any extra words, sentences, explanations, or restatements.\n",
            "- Do not include prefixes like “The answer is:”.\n",
            "- Do not add punctuation at the end (e.g., no trailing period) unless it is part of the answer itself.\n",
            "- Preserve capitalization as it appears in the Context.\n",
            "- Prefer an exact extractive span from the Context; avoid paraphrasing.\n",
            "\n",
            "Guidance by question type:\n",
            "- “When” questions: output the date exactly as written in the Context (e.g., “July 20, 1969”).\n",
            "- “Who” questions: output the person’s name only (e.g., “Gustave Eiffel”).\n",
            "- “Which/What” entity questions: output the entity name only (e.g., “The Amazon River”).\n",
            "\n",
            "Important:\n",
            "- Do not include any additional contextual details (even if true), such as descriptions or qualifications (e.g., avoid appending “by discharge volume of water in the world”).\n",
            "- Use only the provided Context; do not rely on external knowledge.\n",
            "- If the answer is not present in the Context, respond with: Unknown\n",
            "\n",
            "\n",
            "===AVERAGE SCORE===\n",
            "0.97\n",
            "---- Iteration 4 ----\n",
            "===PROMPT===\n",
            "You will receive inputs in this exact format:\n",
            "Context: {context}\n",
            "Question: {question}\n",
            "Answer:\n",
            "\n",
            "Task:\n",
            "Return the shortest, direct answer to the Question using only information from the Context. Prefer copying the exact minimal phrase from the Context.\n",
            "\n",
            "Strict output rules:\n",
            "- Output exactly one line in the form: Answer: {answer}\n",
            "- Do not add any extra words, sentences, explanations, or restatements.\n",
            "- Do not include prefixes like “The answer is”.\n",
            "- Do not add punctuation at the end unless it is part of the answer itself.\n",
            "- Preserve capitalization exactly as it appears in the Context.\n",
            "- Use only the provided Context; do not rely on external knowledge.\n",
            "- If the answer is not present or cannot be determined from the Context, output: Answer: Unknown\n",
            "\n",
            "Answer selection rules:\n",
            "- Prefer an exact extractive span from the Context; avoid paraphrasing.\n",
            "- Choose the shortest span that fully answers the question.\n",
            "- Keep the span contiguous when possible. You may drop surrounding hedges or qualifiers that are not needed for the direct answer (e.g., remove “widely regarded as” if not essential).\n",
            "- Do not introduce new content or change meaning.\n",
            "\n",
            "Allowed minimal adjustment (only when necessary for grammaticality):\n",
            "- For “known for/role/status” questions (e.g., “What is X known for?”), you may prepend “Being” to an otherwise extractive noun phrase to form a grammatical fragment (e.g., “Being the …”). Do not add any other words.\n",
            "- No other paraphrasing or additions are allowed.\n",
            "\n",
            "Guidance by question type:\n",
            "- When questions: output the date exactly as written in the Context (e.g., “July 20, 1969”).\n",
            "- Who questions: output the person’s name only, exactly as in the Context (include initials or middle names if present).\n",
            "- Which/What entity questions: output the entity name only, including any leading article present in the Context (e.g., “The Amazon River”).\n",
            "- Where questions: output the location phrase only, exactly as written.\n",
            "\n",
            "Formatting reminders:\n",
            "- Single line only.\n",
            "- Exactly one “Answer:” prefix.\n",
            "- No trailing punctuation unless it appears in the Context’s answer span.\n",
            "\n",
            "\n",
            "===AVERAGE SCORE===\n",
            "0.99\n"
          ]
        }
      ],
      "source": [
        "for idx, hist in enumerate(results.history):\n",
        "  print(f\"---- Iteration {idx+1} ----\")\n",
        "  print(f\"===PROMPT===\\n{hist.prompt}\")\n",
        "  print(f\"\\n\\n===AVERAGE SCORE===\\n{hist.average_score}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6048fa0c",
      "metadata": {},
      "source": [
        "## Try our other Optimizers!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33890fd1",
      "metadata": {
        "cellView": "form",
        "id": "33890fd1"
      },
      "outputs": [],
      "source": [
        "from fi.opt.optimizers import (\n",
        "    RandomSearchOptimizer,\n",
        "    ProTeGi,\n",
        "    MetaPromptOptimizer,\n",
        "    PromptWizardOptimizer,\n",
        "    BayesianSearchOptimizer,\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
